"""
æ‰¹é‡è¯„ä¼°è„šæœ¬
è¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„ AUCã€å„ç±»åˆ« P/R/F1ã€æ•´ä½“ P/R/F1
"""

import sys
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    f1_score,
    precision_score,
    recall_score,
    roc_auc_score,
)

# ç¡®ä¿å¯ä»¥å¯¼å…¥ ultralytics
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from ultralytics import YOLO


def evaluate_single_model(
    model_path: Path,
    data_path: str,
    imgsz: int = 256,
    batch: int = 32,
    device: str = "0",
) -> dict:
    """
    è¯„ä¼°å•ä¸ªæ¨¡å‹

    Args:
        model_path: best.pt æ¨¡å‹è·¯å¾„
        data_path: æ•°æ®é›†è·¯å¾„
        imgsz: å›¾åƒå°ºå¯¸
        batch: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡

    Returns:
        dict: åŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    try:
        # åŠ è½½æ¨¡å‹
        model = YOLO(str(model_path))

        # è¿è¡ŒéªŒè¯
        results = model.val(
            data=data_path,
            imgsz=imgsz,
            batch=batch,
            device=device,
            split="test",  # ä½¿ç”¨æµ‹è¯•é›†
            verbose=False,
        )

        # è·å–é¢„æµ‹ç»“æœç”¨äºè®¡ç®— AUC
        # éœ€è¦é‡æ–°é¢„æµ‹ä»¥è·å–æ¦‚ç‡
        from ultralytics.data.build import build_classification_dataloader
        from ultralytics.data.utils import check_cls_dataset

        # è·å–æµ‹è¯•é›†è·¯å¾„
        dataset_info = check_cls_dataset(data_path)
        test_path = dataset_info.get("test", dataset_info.get("val"))

        # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
        y_true = []
        y_pred = []
        y_probs = []

        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹æµ‹è¯•é›†
        import os

        for class_idx, class_name in enumerate(sorted(os.listdir(test_path))):
            class_dir = Path(test_path) / class_name
            if not class_dir.is_dir():
                continue

            for img_path in class_dir.glob("*"):
                if img_path.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]:
                    pred_results = model.predict(str(img_path), verbose=False)
                    if pred_results and pred_results[0].probs is not None:
                        probs = pred_results[0].probs.data.cpu().numpy()
                        pred_class = probs.argmax()

                        y_true.append(class_idx)
                        y_pred.append(pred_class)
                        y_probs.append(probs)

        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        y_probs = np.array(y_probs)

        # è·å–ç±»åˆ«åç§°
        class_names = sorted(os.listdir(test_path))
        class_names = [n for n in class_names if (Path(test_path) / n).is_dir()]
        n_classes = len(class_names)

        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        precision_macro = precision_score(y_true, y_pred, average="macro", zero_division=0)
        recall_macro = recall_score(y_true, y_pred, average="macro", zero_division=0)
        f1_macro = f1_score(y_true, y_pred, average="macro", zero_division=0)

        precision_weighted = precision_score(y_true, y_pred, average="weighted", zero_division=0)
        recall_weighted = recall_score(y_true, y_pred, average="weighted", zero_division=0)
        f1_weighted = f1_score(y_true, y_pred, average="weighted", zero_division=0)

        # è®¡ç®— AUC
        if n_classes == 2:
            # äºŒåˆ†ç±» AUC
            auc = roc_auc_score(y_true, y_probs[:, 1])
        else:
            # å¤šåˆ†ç±» AUC (OvR)
            try:
                auc = roc_auc_score(y_true, y_probs, multi_class="ovr", average="macro")
            except ValueError:
                auc = None

        # è®¡ç®—å„ç±»åˆ«æŒ‡æ ‡
        report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)

        # æ„å»ºç»“æœå­—å…¸
        result = {
            "model_path": str(model_path),
            "accuracy": accuracy,
            "auc": auc,
            "precision_macro": precision_macro,
            "recall_macro": recall_macro,
            "f1_macro": f1_macro,
            "precision_weighted": precision_weighted,
            "recall_weighted": recall_weighted,
            "f1_weighted": f1_weighted,
            "top1_accuracy": getattr(results, "top1", accuracy),
            "top5_accuracy": getattr(results, "top5", 1.0),
        }

        # æ·»åŠ å„ç±»åˆ«æŒ‡æ ‡
        for class_name in class_names:
            if class_name in report:
                result[f"{class_name}_precision"] = report[class_name]["precision"]
                result[f"{class_name}_recall"] = report[class_name]["recall"]
                result[f"{class_name}_f1"] = report[class_name]["f1-score"]
                result[f"{class_name}_support"] = report[class_name]["support"]

        return result

    except Exception as e:
        print(f"è¯„ä¼°æ¨¡å‹ {model_path} å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return {"model_path": str(model_path), "error": str(e)}


def batch_evaluate(
    sweep_dir: str | Path,
    data_path: str,
    output_dir: str | Path = None,
    imgsz: int = 256,
    batch: int = 32,
    device: str = "0",
) -> pd.DataFrame:
    """
    æ‰¹é‡è¯„ä¼°æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰å®éªŒçš„ best.pt æ¨¡å‹

    Args:
        sweep_dir: è¶…å‚æ•°æœç´¢ç»“æœç›®å½• (å¦‚ runs/classify/plaque_sweep_n)
        data_path: æ•°æ®é›†è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º sweep_dir
        imgsz: å›¾åƒå°ºå¯¸
        batch: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡

    Returns:
        DataFrame: åŒ…å«æ‰€æœ‰æ¨¡å‹è¯„ä¼°ç»“æœçš„è¡¨æ ¼
    """
    sweep_dir = Path(sweep_dir)
    output_dir = Path(output_dir) if output_dir else sweep_dir

    # æŸ¥æ‰¾æ‰€æœ‰ best.pt æ–‡ä»¶
    model_paths = list(sweep_dir.glob("*/weights/best.pt"))
    print(f"æ‰¾åˆ° {len(model_paths)} ä¸ªæ¨¡å‹å¾…è¯„ä¼°")

    if not model_paths:
        print(f"åœ¨ {sweep_dir} ä¸‹æœªæ‰¾åˆ°ä»»ä½• best.pt æ–‡ä»¶")
        return pd.DataFrame()

    results = []

    for idx, model_path in enumerate(model_paths):
        exp_name = model_path.parent.parent.name
        print(f"\n[{idx + 1}/{len(model_paths)}] è¯„ä¼°: {exp_name}")

        # è¯„ä¼°æ¨¡å‹
        result = evaluate_single_model(
            model_path=model_path,
            data_path=data_path,
            imgsz=imgsz,
            batch=batch,
            device=device,
        )
        result["experiment"] = exp_name

        # è§£æè¶…å‚æ•°åç§°
        # æ ¼å¼: lr00_dropout0_weight_decay0.001
        parts = exp_name.split("_")
        for part in parts:
            if part.startswith("lr0"):
                result["lr0"] = float(part.replace("lr0", "") or "0")
            elif part.startswith("dropout"):
                result["dropout"] = float(part.replace("dropout", ""))
            elif part.startswith("decay"):
                result["weight_decay"] = float(part.replace("decay", ""))
            elif "weight" in exp_name and "decay" in part:
                # å¤„ç† weight_decay æ ¼å¼
                pass

        # ä»å®éªŒåç§°æå– weight_decay
        if "weight_decay" in exp_name:
            wd_part = exp_name.split("weight_decay")[-1]
            try:
                result["weight_decay"] = float(wd_part)
            except ValueError:
                pass

        results.append(result)

        # ä¿å­˜å•ä¸ªæ¨¡å‹çš„è¯¦ç»†ç»“æœ
        exp_result_path = model_path.parent.parent / "test_evaluation.txt"
        with open(exp_result_path, "w", encoding="utf-8") as f:
            f.write(f"å®éªŒåç§°: {exp_name}\n")
            f.write(f"æ¨¡å‹è·¯å¾„: {model_path}\n")
            f.write("=" * 60 + "\n")
            for key, value in result.items():
                if isinstance(value, float):
                    f.write(f"{key}: {value:.6f}\n")
                else:
                    f.write(f"{key}: {value}\n")
        print(f"  ç»“æœå·²ä¿å­˜è‡³: {exp_result_path}")

    # åˆ›å»º DataFrame
    df = pd.DataFrame(results)

    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    priority_cols = [
        "experiment",
        "lr0",
        "dropout",
        "weight_decay",
        "accuracy",
        "auc",
        "f1_macro",
        "f1_weighted",
        "precision_macro",
        "recall_macro",
        "top1_accuracy",
        "top5_accuracy",
    ]
    other_cols = [c for c in df.columns if c not in priority_cols]
    df = df[[c for c in priority_cols if c in df.columns] + other_cols]

    # ä¿å­˜æ±‡æ€»ç»“æœ
    summary_csv = output_dir / "test_evaluation_summary.csv"
    df.to_csv(summary_csv, index=False, encoding="utf-8")
    print(f"\næ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_csv}")

    # æ‰¾å‡ºæœ€ä¼˜æ¨¡å‹
    if "auc" in df.columns and df["auc"].notna().any():
        best_by_auc = df.loc[df["auc"].idxmax()]
        print("\n" + "=" * 60)
        print("ğŸ† æœ€ä¼˜æ¨¡å‹ (æŒ‰ AUC):")
        print(f"  å®éªŒ: {best_by_auc['experiment']}")
        print(f"  AUC: {best_by_auc['auc']:.6f}")
        if "f1_macro" in best_by_auc:
            print(f"  F1-Macro: {best_by_auc['f1_macro']:.6f}")

    if "f1_macro" in df.columns:
        best_by_f1 = df.loc[df["f1_macro"].idxmax()]
        print("\nğŸ† æœ€ä¼˜æ¨¡å‹ (æŒ‰ F1-Macro):")
        print(f"  å®éªŒ: {best_by_f1['experiment']}")
        print(f"  F1-Macro: {best_by_f1['f1_macro']:.6f}")
        if "auc" in best_by_f1 and best_by_f1["auc"] is not None:
            print(f"  AUC: {best_by_f1['auc']:.6f}")

    if "accuracy" in df.columns:
        best_by_acc = df.loc[df["accuracy"].idxmax()]
        print("\nğŸ† æœ€ä¼˜æ¨¡å‹ (æŒ‰ Accuracy):")
        print(f"  å®éªŒ: {best_by_acc['experiment']}")
        print(f"  Accuracy: {best_by_acc['accuracy']:.6f}")

    # ä¿å­˜æœ€ä¼˜æ¨¡å‹ä¿¡æ¯
    best_model_path = output_dir / "best_models.txt"
    with open(best_model_path, "w", encoding="utf-8") as f:
        f.write("=" * 60 + "\n")
        f.write("æœ€ä¼˜æ¨¡å‹æ±‡æ€»\n")
        f.write("=" * 60 + "\n\n")

        if "auc" in df.columns and df["auc"].notna().any():
            best = df.loc[df["auc"].idxmax()]
            f.write("ã€æœ€ä¼˜ AUCã€‘\n")
            for key, value in best.items():
                if isinstance(value, float):
                    f.write(f"  {key}: {value:.6f}\n")
                else:
                    f.write(f"  {key}: {value}\n")
            f.write("\n")

        if "f1_macro" in df.columns:
            best = df.loc[df["f1_macro"].idxmax()]
            f.write("ã€æœ€ä¼˜ F1-Macroã€‘\n")
            for key, value in best.items():
                if isinstance(value, float):
                    f.write(f"  {key}: {value:.6f}\n")
                else:
                    f.write(f"  {key}: {value}\n")
            f.write("\n")

        if "accuracy" in df.columns:
            best = df.loc[df["accuracy"].idxmax()]
            f.write("ã€æœ€ä¼˜ Accuracyã€‘\n")
            for key, value in best.items():
                if isinstance(value, float):
                    f.write(f"  {key}: {value:.6f}\n")
                else:
                    f.write(f"  {key}: {value}\n")

    print(f"\næœ€ä¼˜æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜è‡³: {best_model_path}")

    return df


def evaluate_all_sweeps(
    base_dir: str = "runs/classify",
    data_path: str = "/data/users/lxing/File/medicalImg/CAS/selected_folders/plaque/dataset",
    sweep_patterns: list = None,
    **kwargs,
):
    """
    è¯„ä¼°æ‰€æœ‰è¶…å‚æ•°æœç´¢ç›®å½•

    Args:
        base_dir: åŸºç¡€ç›®å½•
        data_path: æ•°æ®é›†è·¯å¾„
        sweep_patterns: è¦è¯„ä¼°çš„ç›®å½•æ¨¡å¼åˆ—è¡¨
        **kwargs: ä¼ é€’ç»™ batch_evaluate çš„å…¶ä»–å‚æ•°
    """
    base_dir = Path(base_dir)

    if sweep_patterns is None:
        sweep_patterns = ["plaque_sweep*"]

    all_results = []

    for pattern in sweep_patterns:
        for sweep_dir in base_dir.glob(pattern):
            if sweep_dir.is_dir():
                print("\n" + "=" * 80)
                print(f"è¯„ä¼°ç›®å½•: {sweep_dir}")
                print("=" * 80)

                df = batch_evaluate(sweep_dir=sweep_dir, data_path=data_path, **kwargs)

                if not df.empty:
                    df["sweep_dir"] = sweep_dir.name
                    all_results.append(df)

    if all_results:
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        combined_df = pd.concat(all_results, ignore_index=True)
        combined_csv = base_dir / "all_sweeps_evaluation.csv"
        combined_df.to_csv(combined_csv, index=False, encoding="utf-8")
        print(f"\n\næ‰€æœ‰è¶…å‚æ•°æœç´¢çš„è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {combined_csv}")

        # æ‰¾å‡ºå…¨å±€æœ€ä¼˜
        print("\n" + "=" * 80)
        print("ğŸŒŸ å…¨å±€æœ€ä¼˜æ¨¡å‹")
        print("=" * 80)

        if "auc" in combined_df.columns and combined_df["auc"].notna().any():
            best = combined_df.loc[combined_df["auc"].idxmax()]
            print(f"\nã€å…¨å±€æœ€ä¼˜ AUCã€‘")
            print(f"  ç›®å½•: {best.get('sweep_dir', 'N/A')}")
            print(f"  å®éªŒ: {best['experiment']}")
            print(f"  AUC: {best['auc']:.6f}")

        if "f1_macro" in combined_df.columns:
            best = combined_df.loc[combined_df["f1_macro"].idxmax()]
            print(f"\nã€å…¨å±€æœ€ä¼˜ F1-Macroã€‘")
            print(f"  ç›®å½•: {best.get('sweep_dir', 'N/A')}")
            print(f"  å®éªŒ: {best['experiment']}")
            print(f"  F1-Macro: {best['f1_macro']:.6f}")

        return combined_df

    return pd.DataFrame()


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    DATA_PATH = "/data/users/lxing/File/medicalImg/CAS/selected_folders/plaque/dataset"

    # æ–¹å¼1: è¯„ä¼°å•ä¸ªè¶…å‚æ•°æœç´¢ç›®å½•
    # results = batch_evaluate(
    #     sweep_dir="runs/classify/plaque_sweep_n",
    #     data_path=DATA_PATH,
    #     imgsz=256,
    #     batch=32,
    #     device="0",
    # )

    # æ–¹å¼2: è¯„ä¼°æ‰€æœ‰è¶…å‚æ•°æœç´¢ç›®å½•
    results = evaluate_all_sweeps(
        base_dir="runs/classify",
        data_path=DATA_PATH,
        sweep_patterns=["plaque_sweep_n", "plaque_sweep_m", "plaque_sweep"],
        imgsz=256,
        batch=32,
        device="0",
    )

    # æ‰“å°ç»“æœæ‘˜è¦
    if not results.empty:
        print("\n" + "=" * 80)
        print("è¯„ä¼°å®Œæˆï¼ç»“æœæ‘˜è¦:")
        print("=" * 80)

        # æŒ‰ AUC æ’åºæ˜¾ç¤ºå‰ 10
        if "auc" in results.columns:
            print("\nTop 10 æ¨¡å‹ (æŒ‰ AUC æ’åº):")
            top10 = results.nlargest(10, "auc")[
                ["experiment", "sweep_dir", "auc", "f1_macro", "accuracy"]
            ]
            print(top10.to_string(index=False))